{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving and Loading Models\n",
    "\n",
    "In this notebook, I'll show you how to save and load models with PyTorch. This is important because you'll often want to load previously trained models to use in making predictions or to continue training on new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import helper\n",
    "# network structure, train, validation functions\n",
    "import fc_model  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a transform to normalize the data\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.5,), (0.5,))])\n",
    "# Download and load the training data\n",
    "trainset = datasets.FashionMNIST('~/.pytorch/F_MNIST_data/', download=True, train=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Download and load the test data\n",
    "testset = datasets.FashionMNIST('~/.pytorch/F_MNIST_data/', download=True, train=False, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see one of the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 233,
       "width": 233
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "image, label = next(iter(trainloader))\n",
    "helper.imshow(image[0,:]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a network\n",
    "\n",
    "To make things more concise here, I moved the model architecture and training code from the last part to a file called `fc_model`. Importing this, we can easily create a fully-connected network with `fc_model.Network`, and train the network using `fc_model.train`. I'll use this model (once it's trained) to demonstrate how we can save and load models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the network, define the criterion and optimizer\n",
    "\n",
    "model = fc_model.Network(784, 10, [512, 256, 128])\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.parameters of Network(\n",
       "  (hidden_layers): ModuleList(\n",
       "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
       "    (1): Linear(in_features=512, out_features=256, bias=True)\n",
       "    (2): Linear(in_features=256, out_features=128, bias=True)\n",
       "  )\n",
       "  (output): Linear(in_features=128, out_features=10, bias=True)\n",
       "  (dropout): Dropout(p=0.5)\n",
       ")>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/2..  Training Loss: 1.741..  Test Loss: 1.020..  Test Accuracy: 0.648\n",
      "Epoch: 1/2..  Training Loss: 1.025..  Test Loss: 0.759..  Test Accuracy: 0.724\n",
      "Epoch: 1/2..  Training Loss: 0.877..  Test Loss: 0.692..  Test Accuracy: 0.740\n",
      "Epoch: 1/2..  Training Loss: 0.767..  Test Loss: 0.641..  Test Accuracy: 0.755\n",
      "Epoch: 1/2..  Training Loss: 0.776..  Test Loss: 0.661..  Test Accuracy: 0.751\n",
      "Epoch: 1/2..  Training Loss: 0.713..  Test Loss: 0.591..  Test Accuracy: 0.778\n",
      "Epoch: 1/2..  Training Loss: 0.704..  Test Loss: 0.572..  Test Accuracy: 0.777\n",
      "Epoch: 1/2..  Training Loss: 0.656..  Test Loss: 0.562..  Test Accuracy: 0.790\n",
      "Epoch: 1/2..  Training Loss: 0.658..  Test Loss: 0.554..  Test Accuracy: 0.800\n",
      "Epoch: 1/2..  Training Loss: 0.637..  Test Loss: 0.590..  Test Accuracy: 0.786\n",
      "Epoch: 1/2..  Training Loss: 0.626..  Test Loss: 0.517..  Test Accuracy: 0.807\n",
      "Epoch: 1/2..  Training Loss: 0.643..  Test Loss: 0.529..  Test Accuracy: 0.809\n",
      "Epoch: 1/2..  Training Loss: 0.578..  Test Loss: 0.511..  Test Accuracy: 0.809\n",
      "Epoch: 1/2..  Training Loss: 0.622..  Test Loss: 0.517..  Test Accuracy: 0.806\n",
      "Epoch: 1/2..  Training Loss: 0.646..  Test Loss: 0.518..  Test Accuracy: 0.813\n",
      "Epoch: 1/2..  Training Loss: 0.567..  Test Loss: 0.503..  Test Accuracy: 0.819\n",
      "Epoch: 1/2..  Training Loss: 0.593..  Test Loss: 0.495..  Test Accuracy: 0.818\n",
      "Epoch: 1/2..  Training Loss: 0.571..  Test Loss: 0.498..  Test Accuracy: 0.820\n",
      "Epoch: 1/2..  Training Loss: 0.563..  Test Loss: 0.480..  Test Accuracy: 0.829\n",
      "Epoch: 1/2..  Training Loss: 0.551..  Test Loss: 0.521..  Test Accuracy: 0.808\n",
      "Epoch: 1/2..  Training Loss: 0.571..  Test Loss: 0.510..  Test Accuracy: 0.816\n",
      "Epoch: 1/2..  Training Loss: 0.561..  Test Loss: 0.482..  Test Accuracy: 0.830\n",
      "Epoch: 1/2..  Training Loss: 0.589..  Test Loss: 0.488..  Test Accuracy: 0.818\n",
      "Epoch: 2/2..  Training Loss: 0.564..  Test Loss: 0.477..  Test Accuracy: 0.823\n",
      "Epoch: 2/2..  Training Loss: 0.530..  Test Loss: 0.475..  Test Accuracy: 0.828\n",
      "Epoch: 2/2..  Training Loss: 0.549..  Test Loss: 0.496..  Test Accuracy: 0.824\n",
      "Epoch: 2/2..  Training Loss: 0.490..  Test Loss: 0.466..  Test Accuracy: 0.832\n",
      "Epoch: 2/2..  Training Loss: 0.546..  Test Loss: 0.467..  Test Accuracy: 0.826\n",
      "Epoch: 2/2..  Training Loss: 0.555..  Test Loss: 0.471..  Test Accuracy: 0.827\n",
      "Epoch: 2/2..  Training Loss: 0.531..  Test Loss: 0.467..  Test Accuracy: 0.829\n",
      "Epoch: 2/2..  Training Loss: 0.570..  Test Loss: 0.475..  Test Accuracy: 0.830\n",
      "Epoch: 2/2..  Training Loss: 0.563..  Test Loss: 0.480..  Test Accuracy: 0.824\n",
      "Epoch: 2/2..  Training Loss: 0.537..  Test Loss: 0.452..  Test Accuracy: 0.833\n",
      "Epoch: 2/2..  Training Loss: 0.511..  Test Loss: 0.466..  Test Accuracy: 0.826\n",
      "Epoch: 2/2..  Training Loss: 0.548..  Test Loss: 0.481..  Test Accuracy: 0.829\n",
      "Epoch: 2/2..  Training Loss: 0.542..  Test Loss: 0.457..  Test Accuracy: 0.831\n",
      "Epoch: 2/2..  Training Loss: 0.531..  Test Loss: 0.465..  Test Accuracy: 0.825\n",
      "Epoch: 2/2..  Training Loss: 0.520..  Test Loss: 0.453..  Test Accuracy: 0.838\n",
      "Epoch: 2/2..  Training Loss: 0.487..  Test Loss: 0.454..  Test Accuracy: 0.834\n",
      "Epoch: 2/2..  Training Loss: 0.519..  Test Loss: 0.454..  Test Accuracy: 0.835\n",
      "Epoch: 2/2..  Training Loss: 0.562..  Test Loss: 0.456..  Test Accuracy: 0.831\n",
      "Epoch: 2/2..  Training Loss: 0.524..  Test Loss: 0.440..  Test Accuracy: 0.838\n",
      "Epoch: 2/2..  Training Loss: 0.500..  Test Loss: 0.436..  Test Accuracy: 0.836\n",
      "Epoch: 2/2..  Training Loss: 0.519..  Test Loss: 0.467..  Test Accuracy: 0.828\n",
      "Epoch: 2/2..  Training Loss: 0.546..  Test Loss: 0.445..  Test Accuracy: 0.833\n",
      "Epoch: 2/2..  Training Loss: 0.527..  Test Loss: 0.453..  Test Accuracy: 0.842\n"
     ]
    }
   ],
   "source": [
    "fc_model.train(model, trainloader, testloader, criterion, optimizer, epochs=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving and loading networks\n",
    "\n",
    "As you can imagine, it's impractical to train a network every time you need to use it. Instead, we can save trained networks then load them later to train more or use them for predictions.\n",
    "\n",
    "**The parameters for PyTorch networks are stored in a model's `state_dict`. We can see the state dict contains the weight and bias matrices for each of our layers.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our model: \n",
      "\n",
      " Network(\n",
      "  (hidden_layers): ModuleList(\n",
      "    (0): Linear(in_features=784, out_features=400, bias=True)\n",
      "    (1): Linear(in_features=400, out_features=200, bias=True)\n",
      "    (2): Linear(in_features=200, out_features=100, bias=True)\n",
      "  )\n",
      "  (output): Linear(in_features=100, out_features=10, bias=True)\n",
      "  (dropout): Dropout(p=0.5)\n",
      ") \n",
      "\n",
      "The state dict keys: \n",
      "\n",
      " odict_keys(['hidden_layers.0.weight', 'hidden_layers.0.bias', 'hidden_layers.1.weight', 'hidden_layers.1.bias', 'hidden_layers.2.weight', 'hidden_layers.2.bias', 'output.weight', 'output.bias'])\n"
     ]
    }
   ],
   "source": [
    "print(\"Our model: \\n\\n\", model, '\\n')\n",
    "print(\"The state dict keys: \\n\\n\", model.state_dict().keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simplest thing to do is simply **save the state dict with `torch.save`.** For example, we can save it to a file `'checkpoint.pth'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'checkpoint.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can load the state dict with `torch.load`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['hidden_layers.0.weight', 'hidden_layers.0.bias', 'hidden_layers.1.weight', 'hidden_layers.1.bias', 'hidden_layers.2.weight', 'hidden_layers.2.bias', 'output.weight', 'output.bias'])\n"
     ]
    }
   ],
   "source": [
    "state_dict = torch.load('checkpoint.pth')\n",
    "print(state_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('hidden_layers.0.weight',\n",
       "              tensor([[ 3.5232e-02, -9.1174e-03,  2.4658e-02,  ..., -1.2332e-02,\n",
       "                        2.0384e-03, -9.6368e-03],\n",
       "                      [ 2.9793e-02,  2.3250e-02, -1.5024e-02,  ..., -6.7139e-03,\n",
       "                       -8.4312e-03, -2.7729e-02],\n",
       "                      [-1.3306e-02,  3.2886e-02,  3.2726e-02,  ..., -2.4201e-02,\n",
       "                       -1.0418e-02,  3.4541e-02],\n",
       "                      ...,\n",
       "                      [ 3.2910e-02,  3.3502e-03, -2.5022e-03,  ..., -2.3392e-02,\n",
       "                        3.1406e-02, -4.0157e-03],\n",
       "                      [ 1.2426e-02, -1.7211e-03,  9.1556e-03,  ...,  3.1175e-02,\n",
       "                       -1.2066e-02, -7.2984e-03],\n",
       "                      [ 1.9317e-02, -1.0932e-02,  3.1935e-02,  ..., -2.8815e-02,\n",
       "                        3.3329e-02, -2.3306e-02]])),\n",
       "             ('hidden_layers.0.bias', tensor(1.00000e-02 *\n",
       "                     [-3.1804,  0.6588, -1.7855, -1.5009, -2.9371,  3.3828, -1.5564,\n",
       "                      -2.0440,  1.6302,  1.5200,  3.2324,  2.0599,  1.2383, -3.0142,\n",
       "                      -0.9032, -3.0066, -3.0725,  2.7651,  0.6728, -1.1093, -1.2636,\n",
       "                      -2.2667,  2.4743, -0.1738,  1.1829, -1.8093, -0.3283, -1.1233,\n",
       "                       1.5922,  1.7376,  1.2742,  0.9870, -1.2430, -1.6257, -1.1473,\n",
       "                      -2.4716,  0.4651, -1.9032,  1.6480,  2.1670, -1.0053, -2.7263,\n",
       "                      -3.3051, -1.1673,  2.2934,  1.0215, -1.4855, -2.9867,  0.4418,\n",
       "                      -1.3695,  1.9981,  2.4175,  2.4057,  1.2945, -3.2648,  2.0305,\n",
       "                       2.6036,  1.9843,  3.3050,  1.0008,  0.4078, -1.9379, -3.0395,\n",
       "                       3.0209,  2.7078,  0.9752, -2.9568, -0.2870, -0.4744,  3.2909,\n",
       "                      -0.2117, -1.5096, -0.2020,  3.2169,  1.3328, -0.7706, -0.3623,\n",
       "                       0.5623,  2.9856, -1.4420,  1.8298, -3.4621,  1.0548, -3.5050,\n",
       "                       2.5502, -3.1797,  2.6022,  1.4886,  2.1831, -2.6109, -1.0723,\n",
       "                      -3.1097,  0.2471,  2.4290,  0.4021,  2.4039, -1.8767, -1.7004,\n",
       "                       0.7622,  2.5607,  0.3302,  1.8973,  0.1378, -2.1271, -1.7958,\n",
       "                      -2.7569,  2.6951,  1.4303,  3.4152,  0.1003, -1.2878,  2.9024,\n",
       "                       0.5371, -1.2387,  3.3800, -0.4459, -3.5496,  1.8596, -2.6014,\n",
       "                      -1.6486,  1.6034,  1.0045,  1.2416, -0.6560, -1.1710,  2.2930,\n",
       "                       3.2048,  1.7691,  0.3920,  0.8266, -1.9921,  3.0107,  0.7892,\n",
       "                      -0.1521,  3.5246,  3.3158,  2.0495, -2.5700,  0.4403,  2.6330,\n",
       "                      -1.9516, -1.8165,  0.8438,  3.3988, -1.0842,  3.2145, -2.2439,\n",
       "                       3.0252, -0.8203, -3.1830,  0.6170,  1.5248,  1.1682,  0.3577,\n",
       "                       0.3772, -2.1888,  3.3860, -2.4869,  3.4467, -2.4225,  2.2552,\n",
       "                      -1.9944, -3.5333, -2.1644, -2.4059, -2.2760,  3.0351, -2.0452,\n",
       "                      -1.8357, -1.2190, -1.2621, -2.2812,  2.0463,  2.4874,  1.9191,\n",
       "                      -2.4625, -1.3781,  1.4522, -2.2088, -3.3562,  2.7987, -1.4974,\n",
       "                      -1.7233,  2.7500, -0.0963,  0.6532, -3.2681, -1.7620, -2.9007,\n",
       "                       0.3442, -1.0092, -0.0497, -0.4875,  3.1863, -1.0750, -3.0133,\n",
       "                       1.3376, -0.6993,  2.2535,  1.5336,  1.9432,  0.4080,  1.2305,\n",
       "                       2.0010, -0.2402,  0.5309,  1.7007,  0.3951, -0.7077,  0.0084,\n",
       "                      -1.9311,  3.0415,  0.1770,  3.2086, -0.6893,  2.6805, -3.4893,\n",
       "                       0.3680, -0.9590,  3.5156, -0.8328,  0.4406,  1.0217,  0.2806,\n",
       "                      -1.0207,  0.4564, -2.5742,  2.8489,  2.5470, -3.5652,  1.6766,\n",
       "                      -0.1015,  0.8397, -3.1817, -1.9744,  2.8310,  2.3590,  0.3164,\n",
       "                       0.9240, -3.3463, -2.9337,  3.4781,  3.0742, -3.4366,  1.1045,\n",
       "                      -0.0154,  0.4966,  1.9456, -0.8255,  3.2180,  3.3345, -1.4187,\n",
       "                       1.6115, -1.3886, -0.1110, -0.1798, -1.3288, -1.2599,  1.3516,\n",
       "                      -0.3862,  1.3425,  2.1073,  2.0195, -2.4114,  0.0831, -3.3039,\n",
       "                       0.7660, -3.2898,  3.4018,  1.9551, -2.6696, -1.2045, -1.6567,\n",
       "                      -1.4815,  1.0879,  1.6881, -2.6339, -0.7111,  1.6842,  1.8599,\n",
       "                      -2.8969, -2.9187, -1.0113, -1.2348, -0.1311, -3.5632,  0.1030,\n",
       "                      -1.4530, -0.5634, -0.8918, -1.3510, -0.3699,  1.3056,  1.3767,\n",
       "                      -0.1851,  0.1012, -1.5341,  0.1840,  2.8799, -0.8698, -3.0086,\n",
       "                      -1.0041,  1.1024, -3.2272,  3.5031, -1.2653,  2.7501, -1.0616,\n",
       "                       2.1583, -3.0190,  1.4418,  0.1087,  1.1547, -1.0405, -2.4838,\n",
       "                      -3.4766, -0.6227,  3.5222,  2.4643, -2.8590, -2.1839, -0.5119,\n",
       "                       1.6726, -0.4263, -2.0899,  0.2098, -1.1733, -1.0952, -1.1030,\n",
       "                       0.9753,  1.7338, -3.4916,  0.6829,  2.2195,  1.8017, -0.6895,\n",
       "                      -0.4870, -2.1202,  2.7146,  0.9230,  0.6411,  2.9282, -3.5698,\n",
       "                       1.2919,  1.9082,  3.3400, -2.5142, -3.5586,  1.3205,  2.4101,\n",
       "                      -0.6355, -2.3389, -3.5012, -2.3350,  1.7970, -1.3482, -0.4951,\n",
       "                       3.5607,  2.8965, -2.5970,  1.2777,  1.8132, -0.4224,  3.3074,\n",
       "                       0.1366,  1.1056,  3.2393,  1.0571, -2.5871,  1.3480, -3.3008,\n",
       "                      -1.9790,  0.2254, -1.3783,  2.0934, -1.8772,  0.0959, -0.5690,\n",
       "                       2.6358, -1.3985, -2.1091, -2.6420,  0.7907, -3.4271,  1.6559,\n",
       "                      -1.3150,  3.3597,  2.7128,  3.1302, -2.0787, -0.8745,  1.1402,\n",
       "                       3.3052,  1.8793, -1.0652,  2.5208,  2.2601, -2.0649,  2.9110,\n",
       "                      -0.2361])),\n",
       "             ('hidden_layers.1.weight',\n",
       "              tensor([[ 1.7555e-02, -3.9737e-02, -2.6679e-03,  ..., -2.9648e-02,\n",
       "                       -3.8979e-02,  2.1370e-02],\n",
       "                      [-3.8787e-02,  2.8235e-02, -1.4087e-02,  ..., -3.4785e-03,\n",
       "                        3.2759e-02,  4.0509e-02],\n",
       "                      [-2.4129e-02,  2.7234e-03,  3.7005e-02,  ...,  3.6085e-02,\n",
       "                        9.9941e-03, -2.0165e-02],\n",
       "                      ...,\n",
       "                      [-2.3832e-03,  2.1769e-02,  1.5669e-03,  ...,  2.1032e-02,\n",
       "                       -3.7119e-02,  3.2494e-02],\n",
       "                      [-3.8964e-02,  4.8041e-02,  1.5648e-02,  ...,  3.9335e-02,\n",
       "                       -2.5159e-02,  3.5151e-02],\n",
       "                      [ 1.3014e-02,  3.3653e-02, -4.5106e-02,  ..., -1.4097e-03,\n",
       "                        3.1700e-02, -9.1504e-03]])),\n",
       "             ('hidden_layers.1.bias', tensor(1.00000e-02 *\n",
       "                     [ 1.5574,  4.5118, -3.3205,  0.3200, -2.5533,  1.2921,  2.1858,\n",
       "                       4.6136, -2.8161,  1.1403,  4.6613, -2.8919, -1.0783,  2.0941,\n",
       "                      -3.0883, -4.2950, -4.0267, -0.0835, -2.8007,  1.0169,  3.9787,\n",
       "                       3.4784, -4.3666,  4.5704, -3.3540,  4.4134, -4.3531, -1.4529,\n",
       "                      -3.8065,  0.9195,  4.3122,  4.4797,  2.5749,  4.5509, -2.9564,\n",
       "                      -3.8660, -0.6527,  3.4692, -0.1273,  2.8974, -3.1128, -2.1055,\n",
       "                      -1.7958,  2.4629,  3.9514,  0.5430, -0.1288, -4.1801,  4.3133,\n",
       "                      -4.0592,  1.6700, -3.8450,  3.7147,  2.8245,  0.3347, -2.3545,\n",
       "                       1.6104, -1.9663, -0.5659, -4.5916, -1.5061, -1.8818, -3.5530,\n",
       "                      -1.3412,  1.1952,  3.7009,  0.0738,  4.2398,  2.0414, -3.9998,\n",
       "                       4.9305,  1.2026,  1.6473,  1.6125,  0.8793,  3.9801, -3.7834,\n",
       "                       4.9021, -1.6531, -0.1937,  1.9321,  0.8631, -3.3332,  0.1671,\n",
       "                      -4.9979, -3.8176, -2.9763, -1.8030, -0.9273, -1.8805, -0.6484,\n",
       "                       4.8318, -3.6782, -1.4895,  0.2549, -4.1454, -2.4445,  4.2663,\n",
       "                       3.3967, -0.2303,  1.3055, -1.6717,  1.7793,  4.9557,  2.1667,\n",
       "                       4.8199, -4.1434,  1.4286, -3.4060, -3.7481, -4.7907,  2.7428,\n",
       "                      -2.1005, -4.4722,  0.9620,  2.8723, -4.7049,  1.0960,  4.6793,\n",
       "                       2.7116,  2.1849,  3.2588,  0.1134, -0.2718,  3.5605,  4.3554,\n",
       "                      -4.5187,  3.4556, -0.6788,  4.4431, -4.9469, -3.8474,  4.8966,\n",
       "                      -0.1611,  0.0123,  4.3219,  4.1928, -0.5672,  1.6386, -0.8790,\n",
       "                       1.1199, -3.3170, -2.4682, -2.6165, -2.7696,  3.6457, -4.4747,\n",
       "                       3.7620,  1.6941, -4.4290, -1.8117, -3.9917,  2.6277, -2.9513,\n",
       "                      -1.0350, -1.9401,  3.1833,  4.4318,  1.5528, -0.2477,  4.2683,\n",
       "                       4.4578, -1.7444, -1.6893,  1.8527, -1.5229,  1.6717,  3.9631,\n",
       "                       4.8142, -3.2533, -4.3334, -1.2249,  0.0910,  2.1441, -0.3290,\n",
       "                       0.1837, -0.5734,  4.0230, -4.8544,  3.3833, -3.0718,  2.8508,\n",
       "                       1.1640,  2.8444,  3.0672,  0.2105,  2.5182,  3.9132,  4.0680,\n",
       "                       0.8021,  0.8611,  0.7627, -0.6135, -4.2658, -3.5546, -4.7460,\n",
       "                      -1.9939,  4.8333,  4.8635,  1.5560])),\n",
       "             ('hidden_layers.2.weight', tensor(1.00000e-02 *\n",
       "                     [[-3.3725,  6.2357, -2.5946,  ...,  5.4182,  6.3343, -5.4298],\n",
       "                      [ 1.3321, -0.2161,  3.5503,  ..., -0.9173,  6.9432,  1.2334],\n",
       "                      [-6.0490,  0.3025, -2.2326,  ...,  2.0655, -1.7791,  4.1479],\n",
       "                      ...,\n",
       "                      [ 4.4533, -3.0790,  3.4682,  ...,  5.9226,  4.7460, -0.5832],\n",
       "                      [-3.4373,  3.4125,  1.3949,  ..., -6.0288, -5.7192,  5.4659],\n",
       "                      [ 6.0289,  6.4113, -7.0248,  ..., -1.4355, -6.1130, -0.4606]])),\n",
       "             ('hidden_layers.2.bias', tensor(1.00000e-02 *\n",
       "                     [-1.8506,  3.2886,  3.9656, -4.4639,  5.1432, -2.4105, -0.0479,\n",
       "                      -4.8219,  2.5243, -6.8423, -7.0597, -3.1257,  0.7045,  6.3932,\n",
       "                       2.6954,  0.5875, -4.7415,  3.3902, -4.9232, -2.3255,  5.1208,\n",
       "                       6.0579,  5.3610, -5.4475, -2.4895,  1.4426,  2.1027, -6.0125,\n",
       "                       3.3238, -0.1645,  2.3634, -3.9320, -2.2759, -3.7962,  4.8015,\n",
       "                      -1.4849,  0.6530,  0.5841, -2.4856,  4.1688,  1.1792, -0.7100,\n",
       "                       2.7653, -0.0096, -5.6270,  4.2147, -3.9356, -2.0564,  2.3958,\n",
       "                      -2.1051, -4.7332, -6.8728,  0.7626,  1.5726,  3.2514, -6.3871,\n",
       "                      -1.1504, -1.7587,  1.2211, -1.0857,  4.4943, -1.1397,  4.3734,\n",
       "                       6.5866,  2.3516, -4.9061, -1.9728, -3.5048,  3.6296,  4.6539,\n",
       "                      -2.8044,  4.4581, -2.2045, -3.0440,  0.5846,  5.6736, -0.6057,\n",
       "                      -4.9347, -3.7768, -4.3532, -4.5878, -5.3436,  2.5789, -3.0369,\n",
       "                      -2.8874,  0.3562,  6.5867, -0.3592, -6.6958,  5.6404, -0.7353,\n",
       "                      -2.2822, -6.6073, -6.0325, -1.7952, -5.8177, -4.3698, -5.6521,\n",
       "                      -6.8500, -2.5637])),\n",
       "             ('output.weight', tensor(1.00000e-02 *\n",
       "                     [[-9.5918, -9.3352, -4.6187,  0.9022, -2.5226, -5.5316,  4.6715,\n",
       "                        4.0842,  6.6420,  2.9481,  2.1947,  2.7863,  2.9776, -8.7637,\n",
       "                       -4.1449, -3.0790,  0.8829,  9.3130, -3.4547,  1.1664, -9.1578,\n",
       "                        9.5780,  6.8438,  7.4021, -7.2446, -5.5371,  1.8224, -4.7917,\n",
       "                       -6.8677,  5.6860,  7.9502, -5.4000,  8.7413, -4.8706,  2.8556,\n",
       "                        9.8578,  2.3300,  0.3960, -0.7109, -8.7899,  6.0954, -0.1603,\n",
       "                       -5.1184,  0.8709, -1.6027,  8.8268, -3.9709, -3.8195, -1.8225,\n",
       "                       -1.2829, -7.7122,  2.8699, -5.9015, -6.5931, -2.5058,  6.8792,\n",
       "                        9.2182,  5.6045,  8.4855,  5.3287,  5.3301, -7.3200,  3.4116,\n",
       "                       -8.8749,  9.8070, -8.4138, -3.9833,  6.4750,  5.6672,  3.9741,\n",
       "                        1.0748,  8.3417, -9.8307,  7.2776,  3.7855, -8.6242, -8.6474,\n",
       "                        2.6190,  5.1876, -5.5752,  6.2175,  4.3334, -6.2400, -6.6650,\n",
       "                        9.2594, -5.7473, -8.7376, -6.2110, -9.6974, -6.3239, -3.2395,\n",
       "                       -8.0414,  7.7054, -2.1631,  1.4980, -5.5855,  3.8821,  2.8211,\n",
       "                       -4.0590, -9.9836],\n",
       "                      [ 8.8527,  5.1913,  7.2957, -6.2853,  7.1258,  5.3193, -7.1616,\n",
       "                       -6.0987,  9.1521,  9.1657,  3.9890,  0.1657, -4.1278, -1.3592,\n",
       "                       -7.9792, -9.2438, -6.4128, -8.5454,  5.9832,  2.5274, -9.7877,\n",
       "                       -9.9381, -7.6767, -6.6785, -0.5909,  4.5926, -0.4785, -6.6911,\n",
       "                       -4.2096, -1.4536, -5.4711, -9.9680,  5.7434,  0.4438,  0.8734,\n",
       "                       -5.2363,  8.4722,  1.1948, -7.4270,  6.7911, -3.8146, -0.0585,\n",
       "                        0.2180, -6.2050, -7.1294,  3.1851, -9.7151, -7.9480, -0.8085,\n",
       "                       -6.0259, -4.4750, -2.6114,  7.6217, -3.3248, -1.0813,  4.0208,\n",
       "                        5.1897, -0.6870, -2.6027,  2.7032, -9.9032, -5.2960, -7.9020,\n",
       "                       -6.1335, -9.3128,  9.0025, -6.1715,  8.8821,  7.5299, -6.2345,\n",
       "                        4.9804, -9.0136, -5.8479, -2.0309,  4.3286, -1.8792,  5.5658,\n",
       "                       -9.6376, -6.0332, -0.0847, -7.6158, -6.7619,  1.2970, -2.7584,\n",
       "                       -2.7141,  4.8131,  1.3640,  1.3081,  4.3775, -3.8759, -4.9336,\n",
       "                        3.5761,  9.8499,  4.9685,  7.2147,  6.4085, -8.6863,  6.2342,\n",
       "                       -6.7015, -3.3767],\n",
       "                      [-6.8127, -5.6400, -0.2016,  4.4094,  4.0994,  6.4076, -2.0109,\n",
       "                        9.6676,  1.6391, -4.0043,  1.2383, -2.0647,  2.8842,  9.7795,\n",
       "                        4.1282,  5.1862,  0.8309,  9.8663, -0.3384, -5.1413,  4.2845,\n",
       "                        5.7252,  0.4491, -4.5069, -6.2124, -3.9677, -9.6817, -2.0776,\n",
       "                       -5.4208, -3.8505,  5.0801,  7.9264, -0.4874,  2.2428, -3.1722,\n",
       "                       -0.5561,  2.5439,  2.2901, -5.1978, -7.3556, -2.7507, -8.0546,\n",
       "                        6.8188, -7.5803, -4.1320, -8.1555, -9.7307,  0.9287, -4.0762,\n",
       "                       -0.7126,  8.9149, -1.1005,  1.2009, -7.6474, -2.4762, -4.0217,\n",
       "                       -8.9064, -0.4919, -7.6356, -7.9643, -3.9487,  3.1493,  7.4607,\n",
       "                        5.3241, -1.7652, -4.8435, -6.1513,  0.8456, -7.7618, -2.9276,\n",
       "                        2.7615, -3.7844, -9.8545, -3.9252, -8.5181,  9.2987, -8.2831,\n",
       "                       -9.2040,  2.8205,  9.0286,  4.5180, -2.8379, -3.4121,  1.9534,\n",
       "                        7.5328,  4.3484, -7.2155,  8.3168,  7.4444,  2.9449,  1.2913,\n",
       "                       -1.9840, -3.8200, -9.0281, -5.5090, -0.3075,  8.3310,  0.4016,\n",
       "                        4.6395, -7.3341],\n",
       "                      [-5.4846, -6.5926,  0.1346,  7.6233,  1.2269, -6.6112, -1.6801,\n",
       "                        2.5350, -9.6792,  6.7485, -3.5562,  8.8447, -4.3354,  1.7194,\n",
       "                        0.4680,  6.8484,  3.6621, -8.2670,  2.5990,  9.7757,  2.9944,\n",
       "                       -8.9451,  8.9733, -8.5077,  5.1856,  8.6935, -7.5738,  9.6705,\n",
       "                        1.3809, -0.6740, -7.3070,  7.9808, -2.1650, -6.0165, -2.7187,\n",
       "                        9.8250, -0.7766,  3.9328, -0.4202, -4.5334,  8.6628,  1.3621,\n",
       "                       -2.4215, -8.7328, -5.0439, -7.9735,  7.4853, -2.7901,  0.2572,\n",
       "                        1.1519,  3.1196,  4.4793, -3.0654, -2.5211, -2.7685, -4.0544,\n",
       "                        3.6894,  5.6150, -1.1521,  7.2920, -0.4743,  6.0909, -1.3008,\n",
       "                        4.3808,  6.0457,  1.2982,  7.3713, -1.1533, -3.4148, -8.6434,\n",
       "                        2.3353, -0.3495,  4.7638, -5.9775,  6.2894, -3.8230, -9.7124,\n",
       "                       -1.6648,  6.6037, -7.0657, -6.5093,  3.9556, -9.5972, -3.8109,\n",
       "                        5.2433, -5.2937,  3.3563,  6.5498, -1.6689, -2.0377, -3.0506,\n",
       "                       -9.9960, -8.4494, -9.0208, -4.2558, -8.2805, -1.4482,  1.4101,\n",
       "                       -7.8885, -0.8219],\n",
       "                      [-4.8381,  6.8908,  9.6691,  4.5421, -4.9194, -8.6468,  0.2004,\n",
       "                        9.2584, -5.2963,  1.4255, -5.3644, -5.5083, -7.5172, -2.9309,\n",
       "                        1.4020, -8.3373,  9.9323, -2.7179, -2.2427, -3.2218, -0.8247,\n",
       "                       -0.6693, -6.7161, -1.2161,  4.5971, -9.9923,  7.8401,  5.9827,\n",
       "                       -6.5304, -0.4251, -8.0130, -0.8249,  1.0668, -8.0762,  2.9266,\n",
       "                        8.3360, -7.1534,  4.3340, -6.1933, -7.7508,  4.9741,  7.4767,\n",
       "                       -5.7105,  5.2959, -9.2309,  7.3141, -8.7875,  5.1440,  0.6266,\n",
       "                       -8.9274,  3.9388, -1.5502,  7.3194,  2.5589,  4.2066, -1.6637,\n",
       "                       -1.5053,  0.9852, -2.4860, -4.6000, -9.9001,  2.5903,  2.7367,\n",
       "                        6.1674,  3.8839,  5.7331,  7.4315,  6.4190, -1.3793, -1.3978,\n",
       "                       -5.7755, -9.5520, -8.3589,  7.3602,  7.5937, -7.9442,  3.6358,\n",
       "                       -0.6599, -4.2803,  4.3563,  5.3026, -8.3686,  2.3059,  1.8328,\n",
       "                        8.2019, -9.6055,  6.0929,  3.5933, -0.1779, -0.6290,  3.0719,\n",
       "                       -8.3256, -7.7500, -6.3966, -2.3986,  5.0138,  3.7359,  8.9985,\n",
       "                        6.2627,  7.6240],\n",
       "                      [-5.6175,  9.6023,  2.4944,  3.4887, -9.8490,  8.9338,  7.2748,\n",
       "                        6.8649, -4.7409, -6.1115, -6.2064,  4.1815,  7.2281,  2.3517,\n",
       "                       -8.1639,  8.1982, -4.1169, -0.5459,  1.1970, -7.8227, -5.9086,\n",
       "                       -4.6453,  2.9353, -0.8697,  8.4765,  9.6249, -6.3511, -5.5185,\n",
       "                        4.0327,  2.1592, -9.6825,  2.3855,  2.3567,  0.1360, -7.4958,\n",
       "                        2.1392, -9.8468,  0.8205, -2.8422,  9.3255,  0.5980,  2.7044,\n",
       "                        7.5773, -8.3005,  8.4603,  7.1874, -8.8411,  9.0553, -5.0222,\n",
       "                       -9.2246, -1.7230, -8.9150,  8.9792, -2.1965,  7.1707,  5.4392,\n",
       "                       -1.6869, -6.0951,  4.4808, -6.3307, -0.3227,  1.6008, -0.4046,\n",
       "                        1.9752,  7.8712, -9.3892,  6.4236, -7.9483, -8.1837, -3.7096,\n",
       "                       -2.7067, -7.9029, -9.3201,  7.2079,  3.7355, -5.1425,  2.7213,\n",
       "                       -7.7607,  2.7701, -8.0361, -7.3311,  9.4778, -3.9626,  1.4107,\n",
       "                        2.3545,  3.9227,  1.9563, -3.9268,  4.1834, -2.6307,  2.5782,\n",
       "                       -3.5960, -5.8684, -7.9032,  2.0240,  7.8003,  7.1522, -5.1852,\n",
       "                        9.6116,  9.9483],\n",
       "                      [-9.6616,  5.8347,  9.2689, -7.7781,  6.3423, -4.1675, -2.7863,\n",
       "                       -9.5557,  5.2357, -7.1067, -1.0475,  9.4794, -2.6824, -8.5859,\n",
       "                        2.6745,  9.9794, -1.3558, -2.5070,  1.2524, -2.5961,  5.9496,\n",
       "                       -9.3942, -0.2604, -6.5703,  3.7484,  5.7019,  1.3881,  2.3384,\n",
       "                       -6.0364, -4.7998, -8.8258,  0.8730,  8.9433,  9.4913,  6.1048,\n",
       "                        4.0633,  0.6650, -1.1061, -2.2867,  2.5750,  0.2151, -7.5844,\n",
       "                       -8.2335,  7.5704, -2.1287,  8.7325, -9.8651, -8.3632,  5.8529,\n",
       "                       -8.5061, -1.9633,  7.0184,  6.2991, -4.1953, -9.2466, -4.1967,\n",
       "                        6.5500, -0.0389,  8.7425, -8.9595,  8.8860,  9.5445, -3.5477,\n",
       "                       -9.7856,  4.0464, -0.9620, -7.8223,  2.2859,  0.3033,  4.8753,\n",
       "                        2.5717,  4.8027, -1.5448,  4.9202,  5.1363, -0.7130, -6.0486,\n",
       "                        8.2242, -3.8193,  5.0746, -4.1737,  4.9067, -6.4600, -3.3281,\n",
       "                        7.6367,  7.9137,  9.2188, -6.8907,  2.3146,  7.8227, -2.9762,\n",
       "                       -9.5220, -8.2988,  2.6607,  2.9305, -5.3375, -2.9787,  7.8296,\n",
       "                       -4.9038,  4.9170],\n",
       "                      [-6.0599, -5.5491,  2.9687, -6.4210, -9.5244, -8.7308, -2.4140,\n",
       "                        5.9830,  4.6976, -9.4773, -6.2098, -1.6865, -2.0027,  6.1794,\n",
       "                       -4.0505, -8.5995, -4.2180,  4.8354,  8.7129, -2.1761,  0.2615,\n",
       "                       -0.5709,  6.0526,  3.1479,  3.4079,  1.3645,  1.8076, -5.0284,\n",
       "                       -0.0366,  0.0296,  9.2082, -9.2569,  5.2654, -3.1631, -0.7115,\n",
       "                        5.1920, -9.3022, -4.5809, -5.9658,  9.5472, -7.9732,  4.1705,\n",
       "                       -1.7466,  7.6704, -5.3792, -4.8050,  5.6332,  5.2821, -7.3533,\n",
       "                       -8.4412, -2.6318,  1.5244, -3.1069, -3.5218, -3.2928,  2.6249,\n",
       "                       -8.9379,  5.1661, -1.4738, -0.4467,  3.9864,  6.0159, -1.4377,\n",
       "                        4.6154,  1.5594, -3.7600, -7.4305,  8.9873,  1.0470, -9.3754,\n",
       "                       -5.9316,  3.6541,  4.2636,  2.6972,  9.4112,  5.2886,  9.7714,\n",
       "                       -3.6675,  8.1490,  8.0490, -1.2620, -8.9102, -8.3692,  2.3428,\n",
       "                       -6.5806,  4.5461, -1.2401, -5.9931, -2.8880,  0.9935, -8.0787,\n",
       "                        6.3836, -1.4067, -5.2798,  1.3477,  2.7418,  0.2399, -4.1840,\n",
       "                        2.2556,  3.9286],\n",
       "                      [ 4.9567,  6.7337,  1.0233, -8.9311, -8.8157, -0.0009, -8.9271,\n",
       "                        0.1529, -7.9495, -0.1694, -2.9700, -9.8346,  1.9028, -6.1343,\n",
       "                        1.1307, -3.7960,  9.6773, -5.2920, -8.4217,  7.3010,  2.5724,\n",
       "                        8.4769,  5.6790, -2.0447, -8.6150,  9.1589, -4.0808,  3.1867,\n",
       "                       -2.5976, -3.4682,  4.4506, -5.2895, -9.6059, -0.1700,  0.4952,\n",
       "                        9.6355, -1.3327, -0.8760, -0.9454, -6.8562,  4.3180, -5.4786,\n",
       "                        5.2676,  2.2141, -1.6474, -4.3306, -6.1045, -1.5274, -5.2854,\n",
       "                        7.8693, -5.6501, -7.7225, -7.2508, -8.8649, -7.5343, -8.9224,\n",
       "                       -8.8888,  2.8711, -8.3053, -8.6701,  5.6263,  9.8022,  4.2973,\n",
       "                        8.9031, -0.6254, -8.7765, -3.0383,  2.7834,  2.0882,  5.7526,\n",
       "                        0.9788, -5.7905, -9.1587,  0.0671, -8.9780, -0.0287,  5.1349,\n",
       "                        4.9069,  6.4290, -2.7156,  1.5193, -5.0277,  1.6448,  1.1216,\n",
       "                        2.8808, -4.1629,  3.4347, -4.0656,  2.6510, -4.9070,  4.1426,\n",
       "                       -5.7657,  2.1411,  7.3516,  0.9436, -0.1633, -2.3007,  7.8194,\n",
       "                        8.8658,  4.2812],\n",
       "                      [ 3.3001, -8.9308, -8.5026, -8.8442, -3.7716, -2.9774,  4.3268,\n",
       "                        4.4021, -9.0399,  7.4469,  5.4289,  6.9796, -8.8773, -6.6610,\n",
       "                        3.2503, -8.0399,  2.8378, -8.9418, -7.3891,  1.4908, -5.3005,\n",
       "                        9.4997,  8.7373,  4.7037,  6.3946,  3.7551,  7.0192, -6.8910,\n",
       "                       -2.8359, -6.5339,  6.0997, -9.8854,  2.1365, -2.0884,  0.2213,\n",
       "                       -3.0836,  8.7841, -7.3499,  4.5653,  5.1986,  0.9251,  6.0704,\n",
       "                        3.6593,  6.4272, -5.8056,  4.3590,  6.6733,  2.5483,  9.1523,\n",
       "                        4.2969,  2.2322,  0.7616,  8.3382, -6.1142, -9.9075, -3.0139,\n",
       "                        2.7900,  7.6499,  8.8085,  1.6220,  7.1999, -1.4864, -3.6597,\n",
       "                       -4.1384,  6.2247,  3.1229,  4.7278,  7.3702, -5.0886,  9.0308,\n",
       "                       -3.9492,  7.9265,  8.2100,  4.0198, -7.1476,  7.9759, -5.8194,\n",
       "                        6.3992, -2.0251, -6.8627,  8.0047, -4.8898, -8.1035,  6.8529,\n",
       "                        9.9021, -8.0369, -5.0325, -3.6979,  1.1689,  5.2919, -9.6395,\n",
       "                       -0.1269,  7.1448, -9.5934,  1.3526, -8.3424,  9.0479, -4.7843,\n",
       "                        3.6890, -5.1910]])),\n",
       "             ('output.bias',\n",
       "              tensor([-0.1009, -0.2194,  0.1669,  0.0456, -0.0865,  0.0160,  0.0349,\n",
       "                      -0.0638, -0.0423, -0.1113]))])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_dict #no network size info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And to load the state dict in to the network, you do `model.load_state_dict(state_dict)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Seems pretty straightforward, but as usual it's a bit more complicated. Loading the state dict works only if the model architecture is exactly the same as the checkpoint architecture. If I create a model with a different architecture, this fails.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try this\n",
    "model = fc_model.Network(784, 10, [400, 200, 100])\n",
    "# This will throw an error because the tensor sizes are wrong!\n",
    "model.load_state_dict(state_dict)\n",
    "\n",
    "\n",
    "#RuntimeError: Error(s) in loading state_dict for Network:\n",
    "#\tWhile copying the parameter named \"hidden_layers.0.weight\", whose dimensions in the model are torch.Size([400, 784]) and whose dimensions in the checkpoint are torch.Size([512, 784]).\n",
    "#\tWhile copying the parameter named \"hidden_layers.0.bias\", whose dimensions in the model are torch.Size([400]) and whose dimensions in the checkpoint are torch.Size([512]).\n",
    "#\tWhile copying the parameter named \"hidden_layers.1.weight\", whose dimensions in the model are torch.Size([200, 400]) and whose dimensions in the checkpoint are torch.Size([256, 512]).\n",
    "#\tWhile copying the parameter named \"hidden_layers.1.bias\", whose dimensions in the model are torch.Size([200]) and whose dimensions in the checkpoint are torch.Size([256]).\n",
    "#\tWhile copying the parameter named \"hidden_layers.2.weight\", whose dimensions in the model are torch.Size([100, 200]) and whose dimensions in the checkpoint are torch.Size([128, 256]).\n",
    "#\tWhile copying the parameter named \"hidden_layers.2.bias\", whose dimensions in the model are torch.Size([100]) and whose dimensions in the checkpoint are torch.Size([128]).\n",
    "#\tWhile copying the parameter named \"output.weight\", whose dimensions in the model are torch.Size([10, 100]) and whose dimensions in the checkpoint are torch.Size([10, 128]).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means we need to rebuild the model exactly as it was when trained. **Information about the model architecture needs to be saved in the checkpoint, along with the state dict.** To do this, you build a dictionary with all the information you need to compeletely rebuild the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModuleList(\n",
       "  (0): Linear(in_features=784, out_features=400, bias=True)\n",
       "  (1): Linear(in_features=400, out_features=200, bias=True)\n",
       "  (2): Linear(in_features=200, out_features=100, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.hidden_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=784, out_features=400, bias=True)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.hidden_layers[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.hidden_layers[0].out_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = {'input_size': 784,\n",
    "              'output_size': 10,\n",
    "              'hidden_layers': [each.out_features for each in model.hidden_layers],\n",
    "              'state_dict': model.state_dict()}\n",
    "\n",
    "torch.save(checkpoint, 'checkpoint.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the checkpoint has all the necessary information to rebuild the trained model. You can easily make that a function if you want. Similarly, we can write a function to load checkpoints. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint(filepath):\n",
    "    # state_dict = torch.load('checkpoint.pth')\n",
    "    # model.load_state_dict(state_dict)\n",
    "    checkpoint = torch.load(filepath)\n",
    "    print(checkpoint)\n",
    "    print('end checkpoint print')\n",
    "    model = fc_model.Network(checkpoint['input_size'],\n",
    "                             checkpoint['output_size'],\n",
    "                             checkpoint['hidden_layers'])\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_size': 784, 'output_size': 10, 'hidden_layers': [400, 200, 100], 'state_dict': OrderedDict([('hidden_layers.0.weight', tensor([[ 3.5232e-02, -9.1174e-03,  2.4658e-02,  ..., -1.2332e-02,\n",
      "          2.0384e-03, -9.6368e-03],\n",
      "        [ 2.9793e-02,  2.3250e-02, -1.5024e-02,  ..., -6.7139e-03,\n",
      "         -8.4312e-03, -2.7729e-02],\n",
      "        [-1.3306e-02,  3.2886e-02,  3.2726e-02,  ..., -2.4201e-02,\n",
      "         -1.0418e-02,  3.4541e-02],\n",
      "        ...,\n",
      "        [ 3.2910e-02,  3.3502e-03, -2.5022e-03,  ..., -2.3392e-02,\n",
      "          3.1406e-02, -4.0157e-03],\n",
      "        [ 1.2426e-02, -1.7211e-03,  9.1556e-03,  ...,  3.1175e-02,\n",
      "         -1.2066e-02, -7.2984e-03],\n",
      "        [ 1.9317e-02, -1.0932e-02,  3.1935e-02,  ..., -2.8815e-02,\n",
      "          3.3329e-02, -2.3306e-02]])), ('hidden_layers.0.bias', tensor(1.00000e-02 *\n",
      "       [-3.1804,  0.6588, -1.7855, -1.5009, -2.9371,  3.3828, -1.5564,\n",
      "        -2.0440,  1.6302,  1.5200,  3.2324,  2.0599,  1.2383, -3.0142,\n",
      "        -0.9032, -3.0066, -3.0725,  2.7651,  0.6728, -1.1093, -1.2636,\n",
      "        -2.2667,  2.4743, -0.1738,  1.1829, -1.8093, -0.3283, -1.1233,\n",
      "         1.5922,  1.7376,  1.2742,  0.9870, -1.2430, -1.6257, -1.1473,\n",
      "        -2.4716,  0.4651, -1.9032,  1.6480,  2.1670, -1.0053, -2.7263,\n",
      "        -3.3051, -1.1673,  2.2934,  1.0215, -1.4855, -2.9867,  0.4418,\n",
      "        -1.3695,  1.9981,  2.4175,  2.4057,  1.2945, -3.2648,  2.0305,\n",
      "         2.6036,  1.9843,  3.3050,  1.0008,  0.4078, -1.9379, -3.0395,\n",
      "         3.0209,  2.7078,  0.9752, -2.9568, -0.2870, -0.4744,  3.2909,\n",
      "        -0.2117, -1.5096, -0.2020,  3.2169,  1.3328, -0.7706, -0.3623,\n",
      "         0.5623,  2.9856, -1.4420,  1.8298, -3.4621,  1.0548, -3.5050,\n",
      "         2.5502, -3.1797,  2.6022,  1.4886,  2.1831, -2.6109, -1.0723,\n",
      "        -3.1097,  0.2471,  2.4290,  0.4021,  2.4039, -1.8767, -1.7004,\n",
      "         0.7622,  2.5607,  0.3302,  1.8973,  0.1378, -2.1271, -1.7958,\n",
      "        -2.7569,  2.6951,  1.4303,  3.4152,  0.1003, -1.2878,  2.9024,\n",
      "         0.5371, -1.2387,  3.3800, -0.4459, -3.5496,  1.8596, -2.6014,\n",
      "        -1.6486,  1.6034,  1.0045,  1.2416, -0.6560, -1.1710,  2.2930,\n",
      "         3.2048,  1.7691,  0.3920,  0.8266, -1.9921,  3.0107,  0.7892,\n",
      "        -0.1521,  3.5246,  3.3158,  2.0495, -2.5700,  0.4403,  2.6330,\n",
      "        -1.9516, -1.8165,  0.8438,  3.3988, -1.0842,  3.2145, -2.2439,\n",
      "         3.0252, -0.8203, -3.1830,  0.6170,  1.5248,  1.1682,  0.3577,\n",
      "         0.3772, -2.1888,  3.3860, -2.4869,  3.4467, -2.4225,  2.2552,\n",
      "        -1.9944, -3.5333, -2.1644, -2.4059, -2.2760,  3.0351, -2.0452,\n",
      "        -1.8357, -1.2190, -1.2621, -2.2812,  2.0463,  2.4874,  1.9191,\n",
      "        -2.4625, -1.3781,  1.4522, -2.2088, -3.3562,  2.7987, -1.4974,\n",
      "        -1.7233,  2.7500, -0.0963,  0.6532, -3.2681, -1.7620, -2.9007,\n",
      "         0.3442, -1.0092, -0.0497, -0.4875,  3.1863, -1.0750, -3.0133,\n",
      "         1.3376, -0.6993,  2.2535,  1.5336,  1.9432,  0.4080,  1.2305,\n",
      "         2.0010, -0.2402,  0.5309,  1.7007,  0.3951, -0.7077,  0.0084,\n",
      "        -1.9311,  3.0415,  0.1770,  3.2086, -0.6893,  2.6805, -3.4893,\n",
      "         0.3680, -0.9590,  3.5156, -0.8328,  0.4406,  1.0217,  0.2806,\n",
      "        -1.0207,  0.4564, -2.5742,  2.8489,  2.5470, -3.5652,  1.6766,\n",
      "        -0.1015,  0.8397, -3.1817, -1.9744,  2.8310,  2.3590,  0.3164,\n",
      "         0.9240, -3.3463, -2.9337,  3.4781,  3.0742, -3.4366,  1.1045,\n",
      "        -0.0154,  0.4966,  1.9456, -0.8255,  3.2180,  3.3345, -1.4187,\n",
      "         1.6115, -1.3886, -0.1110, -0.1798, -1.3288, -1.2599,  1.3516,\n",
      "        -0.3862,  1.3425,  2.1073,  2.0195, -2.4114,  0.0831, -3.3039,\n",
      "         0.7660, -3.2898,  3.4018,  1.9551, -2.6696, -1.2045, -1.6567,\n",
      "        -1.4815,  1.0879,  1.6881, -2.6339, -0.7111,  1.6842,  1.8599,\n",
      "        -2.8969, -2.9187, -1.0113, -1.2348, -0.1311, -3.5632,  0.1030,\n",
      "        -1.4530, -0.5634, -0.8918, -1.3510, -0.3699,  1.3056,  1.3767,\n",
      "        -0.1851,  0.1012, -1.5341,  0.1840,  2.8799, -0.8698, -3.0086,\n",
      "        -1.0041,  1.1024, -3.2272,  3.5031, -1.2653,  2.7501, -1.0616,\n",
      "         2.1583, -3.0190,  1.4418,  0.1087,  1.1547, -1.0405, -2.4838,\n",
      "        -3.4766, -0.6227,  3.5222,  2.4643, -2.8590, -2.1839, -0.5119,\n",
      "         1.6726, -0.4263, -2.0899,  0.2098, -1.1733, -1.0952, -1.1030,\n",
      "         0.9753,  1.7338, -3.4916,  0.6829,  2.2195,  1.8017, -0.6895,\n",
      "        -0.4870, -2.1202,  2.7146,  0.9230,  0.6411,  2.9282, -3.5698,\n",
      "         1.2919,  1.9082,  3.3400, -2.5142, -3.5586,  1.3205,  2.4101,\n",
      "        -0.6355, -2.3389, -3.5012, -2.3350,  1.7970, -1.3482, -0.4951,\n",
      "         3.5607,  2.8965, -2.5970,  1.2777,  1.8132, -0.4224,  3.3074,\n",
      "         0.1366,  1.1056,  3.2393,  1.0571, -2.5871,  1.3480, -3.3008,\n",
      "        -1.9790,  0.2254, -1.3783,  2.0934, -1.8772,  0.0959, -0.5690,\n",
      "         2.6358, -1.3985, -2.1091, -2.6420,  0.7907, -3.4271,  1.6559,\n",
      "        -1.3150,  3.3597,  2.7128,  3.1302, -2.0787, -0.8745,  1.1402,\n",
      "         3.3052,  1.8793, -1.0652,  2.5208,  2.2601, -2.0649,  2.9110,\n",
      "        -0.2361])), ('hidden_layers.1.weight', tensor([[ 1.7555e-02, -3.9737e-02, -2.6679e-03,  ..., -2.9648e-02,\n",
      "         -3.8979e-02,  2.1370e-02],\n",
      "        [-3.8787e-02,  2.8235e-02, -1.4087e-02,  ..., -3.4785e-03,\n",
      "          3.2759e-02,  4.0509e-02],\n",
      "        [-2.4129e-02,  2.7234e-03,  3.7005e-02,  ...,  3.6085e-02,\n",
      "          9.9941e-03, -2.0165e-02],\n",
      "        ...,\n",
      "        [-2.3832e-03,  2.1769e-02,  1.5669e-03,  ...,  2.1032e-02,\n",
      "         -3.7119e-02,  3.2494e-02],\n",
      "        [-3.8964e-02,  4.8041e-02,  1.5648e-02,  ...,  3.9335e-02,\n",
      "         -2.5159e-02,  3.5151e-02],\n",
      "        [ 1.3014e-02,  3.3653e-02, -4.5106e-02,  ..., -1.4097e-03,\n",
      "          3.1700e-02, -9.1504e-03]])), ('hidden_layers.1.bias', tensor(1.00000e-02 *\n",
      "       [ 1.5574,  4.5118, -3.3205,  0.3200, -2.5533,  1.2921,  2.1858,\n",
      "         4.6136, -2.8161,  1.1403,  4.6613, -2.8919, -1.0783,  2.0941,\n",
      "        -3.0883, -4.2950, -4.0267, -0.0835, -2.8007,  1.0169,  3.9787,\n",
      "         3.4784, -4.3666,  4.5704, -3.3540,  4.4134, -4.3531, -1.4529,\n",
      "        -3.8065,  0.9195,  4.3122,  4.4797,  2.5749,  4.5509, -2.9564,\n",
      "        -3.8660, -0.6527,  3.4692, -0.1273,  2.8974, -3.1128, -2.1055,\n",
      "        -1.7958,  2.4629,  3.9514,  0.5430, -0.1288, -4.1801,  4.3133,\n",
      "        -4.0592,  1.6700, -3.8450,  3.7147,  2.8245,  0.3347, -2.3545,\n",
      "         1.6104, -1.9663, -0.5659, -4.5916, -1.5061, -1.8818, -3.5530,\n",
      "        -1.3412,  1.1952,  3.7009,  0.0738,  4.2398,  2.0414, -3.9998,\n",
      "         4.9305,  1.2026,  1.6473,  1.6125,  0.8793,  3.9801, -3.7834,\n",
      "         4.9021, -1.6531, -0.1937,  1.9321,  0.8631, -3.3332,  0.1671,\n",
      "        -4.9979, -3.8176, -2.9763, -1.8030, -0.9273, -1.8805, -0.6484,\n",
      "         4.8318, -3.6782, -1.4895,  0.2549, -4.1454, -2.4445,  4.2663,\n",
      "         3.3967, -0.2303,  1.3055, -1.6717,  1.7793,  4.9557,  2.1667,\n",
      "         4.8199, -4.1434,  1.4286, -3.4060, -3.7481, -4.7907,  2.7428,\n",
      "        -2.1005, -4.4722,  0.9620,  2.8723, -4.7049,  1.0960,  4.6793,\n",
      "         2.7116,  2.1849,  3.2588,  0.1134, -0.2718,  3.5605,  4.3554,\n",
      "        -4.5187,  3.4556, -0.6788,  4.4431, -4.9469, -3.8474,  4.8966,\n",
      "        -0.1611,  0.0123,  4.3219,  4.1928, -0.5672,  1.6386, -0.8790,\n",
      "         1.1199, -3.3170, -2.4682, -2.6165, -2.7696,  3.6457, -4.4747,\n",
      "         3.7620,  1.6941, -4.4290, -1.8117, -3.9917,  2.6277, -2.9513,\n",
      "        -1.0350, -1.9401,  3.1833,  4.4318,  1.5528, -0.2477,  4.2683,\n",
      "         4.4578, -1.7444, -1.6893,  1.8527, -1.5229,  1.6717,  3.9631,\n",
      "         4.8142, -3.2533, -4.3334, -1.2249,  0.0910,  2.1441, -0.3290,\n",
      "         0.1837, -0.5734,  4.0230, -4.8544,  3.3833, -3.0718,  2.8508,\n",
      "         1.1640,  2.8444,  3.0672,  0.2105,  2.5182,  3.9132,  4.0680,\n",
      "         0.8021,  0.8611,  0.7627, -0.6135, -4.2658, -3.5546, -4.7460,\n",
      "        -1.9939,  4.8333,  4.8635,  1.5560])), ('hidden_layers.2.weight', tensor(1.00000e-02 *\n",
      "       [[-3.3725,  6.2357, -2.5946,  ...,  5.4182,  6.3343, -5.4298],\n",
      "        [ 1.3321, -0.2161,  3.5503,  ..., -0.9173,  6.9432,  1.2334],\n",
      "        [-6.0490,  0.3025, -2.2326,  ...,  2.0655, -1.7791,  4.1479],\n",
      "        ...,\n",
      "        [ 4.4533, -3.0790,  3.4682,  ...,  5.9226,  4.7460, -0.5832],\n",
      "        [-3.4373,  3.4125,  1.3949,  ..., -6.0288, -5.7192,  5.4659],\n",
      "        [ 6.0289,  6.4113, -7.0248,  ..., -1.4355, -6.1130, -0.4606]])), ('hidden_layers.2.bias', tensor(1.00000e-02 *\n",
      "       [-1.8506,  3.2886,  3.9656, -4.4639,  5.1432, -2.4105, -0.0479,\n",
      "        -4.8219,  2.5243, -6.8423, -7.0597, -3.1257,  0.7045,  6.3932,\n",
      "         2.6954,  0.5875, -4.7415,  3.3902, -4.9232, -2.3255,  5.1208,\n",
      "         6.0579,  5.3610, -5.4475, -2.4895,  1.4426,  2.1027, -6.0125,\n",
      "         3.3238, -0.1645,  2.3634, -3.9320, -2.2759, -3.7962,  4.8015,\n",
      "        -1.4849,  0.6530,  0.5841, -2.4856,  4.1688,  1.1792, -0.7100,\n",
      "         2.7653, -0.0096, -5.6270,  4.2147, -3.9356, -2.0564,  2.3958,\n",
      "        -2.1051, -4.7332, -6.8728,  0.7626,  1.5726,  3.2514, -6.3871,\n",
      "        -1.1504, -1.7587,  1.2211, -1.0857,  4.4943, -1.1397,  4.3734,\n",
      "         6.5866,  2.3516, -4.9061, -1.9728, -3.5048,  3.6296,  4.6539,\n",
      "        -2.8044,  4.4581, -2.2045, -3.0440,  0.5846,  5.6736, -0.6057,\n",
      "        -4.9347, -3.7768, -4.3532, -4.5878, -5.3436,  2.5789, -3.0369,\n",
      "        -2.8874,  0.3562,  6.5867, -0.3592, -6.6958,  5.6404, -0.7353,\n",
      "        -2.2822, -6.6073, -6.0325, -1.7952, -5.8177, -4.3698, -5.6521,\n",
      "        -6.8500, -2.5637])), ('output.weight', tensor(1.00000e-02 *\n",
      "       [[-9.5918, -9.3352, -4.6187,  0.9022, -2.5226, -5.5316,  4.6715,\n",
      "          4.0842,  6.6420,  2.9481,  2.1947,  2.7863,  2.9776, -8.7637,\n",
      "         -4.1449, -3.0790,  0.8829,  9.3130, -3.4547,  1.1664, -9.1578,\n",
      "          9.5780,  6.8438,  7.4021, -7.2446, -5.5371,  1.8224, -4.7917,\n",
      "         -6.8677,  5.6860,  7.9502, -5.4000,  8.7413, -4.8706,  2.8556,\n",
      "          9.8578,  2.3300,  0.3960, -0.7109, -8.7899,  6.0954, -0.1603,\n",
      "         -5.1184,  0.8709, -1.6027,  8.8268, -3.9709, -3.8195, -1.8225,\n",
      "         -1.2829, -7.7122,  2.8699, -5.9015, -6.5931, -2.5058,  6.8792,\n",
      "          9.2182,  5.6045,  8.4855,  5.3287,  5.3301, -7.3200,  3.4116,\n",
      "         -8.8749,  9.8070, -8.4138, -3.9833,  6.4750,  5.6672,  3.9741,\n",
      "          1.0748,  8.3417, -9.8307,  7.2776,  3.7855, -8.6242, -8.6474,\n",
      "          2.6190,  5.1876, -5.5752,  6.2175,  4.3334, -6.2400, -6.6650,\n",
      "          9.2594, -5.7473, -8.7376, -6.2110, -9.6974, -6.3239, -3.2395,\n",
      "         -8.0414,  7.7054, -2.1631,  1.4980, -5.5855,  3.8821,  2.8211,\n",
      "         -4.0590, -9.9836],\n",
      "        [ 8.8527,  5.1913,  7.2957, -6.2853,  7.1258,  5.3193, -7.1616,\n",
      "         -6.0987,  9.1521,  9.1657,  3.9890,  0.1657, -4.1278, -1.3592,\n",
      "         -7.9792, -9.2438, -6.4128, -8.5454,  5.9832,  2.5274, -9.7877,\n",
      "         -9.9381, -7.6767, -6.6785, -0.5909,  4.5926, -0.4785, -6.6911,\n",
      "         -4.2096, -1.4536, -5.4711, -9.9680,  5.7434,  0.4438,  0.8734,\n",
      "         -5.2363,  8.4722,  1.1948, -7.4270,  6.7911, -3.8146, -0.0585,\n",
      "          0.2180, -6.2050, -7.1294,  3.1851, -9.7151, -7.9480, -0.8085,\n",
      "         -6.0259, -4.4750, -2.6114,  7.6217, -3.3248, -1.0813,  4.0208,\n",
      "          5.1897, -0.6870, -2.6027,  2.7032, -9.9032, -5.2960, -7.9020,\n",
      "         -6.1335, -9.3128,  9.0025, -6.1715,  8.8821,  7.5299, -6.2345,\n",
      "          4.9804, -9.0136, -5.8479, -2.0309,  4.3286, -1.8792,  5.5658,\n",
      "         -9.6376, -6.0332, -0.0847, -7.6158, -6.7619,  1.2970, -2.7584,\n",
      "         -2.7141,  4.8131,  1.3640,  1.3081,  4.3775, -3.8759, -4.9336,\n",
      "          3.5761,  9.8499,  4.9685,  7.2147,  6.4085, -8.6863,  6.2342,\n",
      "         -6.7015, -3.3767],\n",
      "        [-6.8127, -5.6400, -0.2016,  4.4094,  4.0994,  6.4076, -2.0109,\n",
      "          9.6676,  1.6391, -4.0043,  1.2383, -2.0647,  2.8842,  9.7795,\n",
      "          4.1282,  5.1862,  0.8309,  9.8663, -0.3384, -5.1413,  4.2845,\n",
      "          5.7252,  0.4491, -4.5069, -6.2124, -3.9677, -9.6817, -2.0776,\n",
      "         -5.4208, -3.8505,  5.0801,  7.9264, -0.4874,  2.2428, -3.1722,\n",
      "         -0.5561,  2.5439,  2.2901, -5.1978, -7.3556, -2.7507, -8.0546,\n",
      "          6.8188, -7.5803, -4.1320, -8.1555, -9.7307,  0.9287, -4.0762,\n",
      "         -0.7126,  8.9149, -1.1005,  1.2009, -7.6474, -2.4762, -4.0217,\n",
      "         -8.9064, -0.4919, -7.6356, -7.9643, -3.9487,  3.1493,  7.4607,\n",
      "          5.3241, -1.7652, -4.8435, -6.1513,  0.8456, -7.7618, -2.9276,\n",
      "          2.7615, -3.7844, -9.8545, -3.9252, -8.5181,  9.2987, -8.2831,\n",
      "         -9.2040,  2.8205,  9.0286,  4.5180, -2.8379, -3.4121,  1.9534,\n",
      "          7.5328,  4.3484, -7.2155,  8.3168,  7.4444,  2.9449,  1.2913,\n",
      "         -1.9840, -3.8200, -9.0281, -5.5090, -0.3075,  8.3310,  0.4016,\n",
      "          4.6395, -7.3341],\n",
      "        [-5.4846, -6.5926,  0.1346,  7.6233,  1.2269, -6.6112, -1.6801,\n",
      "          2.5350, -9.6792,  6.7485, -3.5562,  8.8447, -4.3354,  1.7194,\n",
      "          0.4680,  6.8484,  3.6621, -8.2670,  2.5990,  9.7757,  2.9944,\n",
      "         -8.9451,  8.9733, -8.5077,  5.1856,  8.6935, -7.5738,  9.6705,\n",
      "          1.3809, -0.6740, -7.3070,  7.9808, -2.1650, -6.0165, -2.7187,\n",
      "          9.8250, -0.7766,  3.9328, -0.4202, -4.5334,  8.6628,  1.3621,\n",
      "         -2.4215, -8.7328, -5.0439, -7.9735,  7.4853, -2.7901,  0.2572,\n",
      "          1.1519,  3.1196,  4.4793, -3.0654, -2.5211, -2.7685, -4.0544,\n",
      "          3.6894,  5.6150, -1.1521,  7.2920, -0.4743,  6.0909, -1.3008,\n",
      "          4.3808,  6.0457,  1.2982,  7.3713, -1.1533, -3.4148, -8.6434,\n",
      "          2.3353, -0.3495,  4.7638, -5.9775,  6.2894, -3.8230, -9.7124,\n",
      "         -1.6648,  6.6037, -7.0657, -6.5093,  3.9556, -9.5972, -3.8109,\n",
      "          5.2433, -5.2937,  3.3563,  6.5498, -1.6689, -2.0377, -3.0506,\n",
      "         -9.9960, -8.4494, -9.0208, -4.2558, -8.2805, -1.4482,  1.4101,\n",
      "         -7.8885, -0.8219],\n",
      "        [-4.8381,  6.8908,  9.6691,  4.5421, -4.9194, -8.6468,  0.2004,\n",
      "          9.2584, -5.2963,  1.4255, -5.3644, -5.5083, -7.5172, -2.9309,\n",
      "          1.4020, -8.3373,  9.9323, -2.7179, -2.2427, -3.2218, -0.8247,\n",
      "         -0.6693, -6.7161, -1.2161,  4.5971, -9.9923,  7.8401,  5.9827,\n",
      "         -6.5304, -0.4251, -8.0130, -0.8249,  1.0668, -8.0762,  2.9266,\n",
      "          8.3360, -7.1534,  4.3340, -6.1933, -7.7508,  4.9741,  7.4767,\n",
      "         -5.7105,  5.2959, -9.2309,  7.3141, -8.7875,  5.1440,  0.6266,\n",
      "         -8.9274,  3.9388, -1.5502,  7.3194,  2.5589,  4.2066, -1.6637,\n",
      "         -1.5053,  0.9852, -2.4860, -4.6000, -9.9001,  2.5903,  2.7367,\n",
      "          6.1674,  3.8839,  5.7331,  7.4315,  6.4190, -1.3793, -1.3978,\n",
      "         -5.7755, -9.5520, -8.3589,  7.3602,  7.5937, -7.9442,  3.6358,\n",
      "         -0.6599, -4.2803,  4.3563,  5.3026, -8.3686,  2.3059,  1.8328,\n",
      "          8.2019, -9.6055,  6.0929,  3.5933, -0.1779, -0.6290,  3.0719,\n",
      "         -8.3256, -7.7500, -6.3966, -2.3986,  5.0138,  3.7359,  8.9985,\n",
      "          6.2627,  7.6240],\n",
      "        [-5.6175,  9.6023,  2.4944,  3.4887, -9.8490,  8.9338,  7.2748,\n",
      "          6.8649, -4.7409, -6.1115, -6.2064,  4.1815,  7.2281,  2.3517,\n",
      "         -8.1639,  8.1982, -4.1169, -0.5459,  1.1970, -7.8227, -5.9086,\n",
      "         -4.6453,  2.9353, -0.8697,  8.4765,  9.6249, -6.3511, -5.5185,\n",
      "          4.0327,  2.1592, -9.6825,  2.3855,  2.3567,  0.1360, -7.4958,\n",
      "          2.1392, -9.8468,  0.8205, -2.8422,  9.3255,  0.5980,  2.7044,\n",
      "          7.5773, -8.3005,  8.4603,  7.1874, -8.8411,  9.0553, -5.0222,\n",
      "         -9.2246, -1.7230, -8.9150,  8.9792, -2.1965,  7.1707,  5.4392,\n",
      "         -1.6869, -6.0951,  4.4808, -6.3307, -0.3227,  1.6008, -0.4046,\n",
      "          1.9752,  7.8712, -9.3892,  6.4236, -7.9483, -8.1837, -3.7096,\n",
      "         -2.7067, -7.9029, -9.3201,  7.2079,  3.7355, -5.1425,  2.7213,\n",
      "         -7.7607,  2.7701, -8.0361, -7.3311,  9.4778, -3.9626,  1.4107,\n",
      "          2.3545,  3.9227,  1.9563, -3.9268,  4.1834, -2.6307,  2.5782,\n",
      "         -3.5960, -5.8684, -7.9032,  2.0240,  7.8003,  7.1522, -5.1852,\n",
      "          9.6116,  9.9483],\n",
      "        [-9.6616,  5.8347,  9.2689, -7.7781,  6.3423, -4.1675, -2.7863,\n",
      "         -9.5557,  5.2357, -7.1067, -1.0475,  9.4794, -2.6824, -8.5859,\n",
      "          2.6745,  9.9794, -1.3558, -2.5070,  1.2524, -2.5961,  5.9496,\n",
      "         -9.3942, -0.2604, -6.5703,  3.7484,  5.7019,  1.3881,  2.3384,\n",
      "         -6.0364, -4.7998, -8.8258,  0.8730,  8.9433,  9.4913,  6.1048,\n",
      "          4.0633,  0.6650, -1.1061, -2.2867,  2.5750,  0.2151, -7.5844,\n",
      "         -8.2335,  7.5704, -2.1287,  8.7325, -9.8651, -8.3632,  5.8529,\n",
      "         -8.5061, -1.9633,  7.0184,  6.2991, -4.1953, -9.2466, -4.1967,\n",
      "          6.5500, -0.0389,  8.7425, -8.9595,  8.8860,  9.5445, -3.5477,\n",
      "         -9.7856,  4.0464, -0.9620, -7.8223,  2.2859,  0.3033,  4.8753,\n",
      "          2.5717,  4.8027, -1.5448,  4.9202,  5.1363, -0.7130, -6.0486,\n",
      "          8.2242, -3.8193,  5.0746, -4.1737,  4.9067, -6.4600, -3.3281,\n",
      "          7.6367,  7.9137,  9.2188, -6.8907,  2.3146,  7.8227, -2.9762,\n",
      "         -9.5220, -8.2988,  2.6607,  2.9305, -5.3375, -2.9787,  7.8296,\n",
      "         -4.9038,  4.9170],\n",
      "        [-6.0599, -5.5491,  2.9687, -6.4210, -9.5244, -8.7308, -2.4140,\n",
      "          5.9830,  4.6976, -9.4773, -6.2098, -1.6865, -2.0027,  6.1794,\n",
      "         -4.0505, -8.5995, -4.2180,  4.8354,  8.7129, -2.1761,  0.2615,\n",
      "         -0.5709,  6.0526,  3.1479,  3.4079,  1.3645,  1.8076, -5.0284,\n",
      "         -0.0366,  0.0296,  9.2082, -9.2569,  5.2654, -3.1631, -0.7115,\n",
      "          5.1920, -9.3022, -4.5809, -5.9658,  9.5472, -7.9732,  4.1705,\n",
      "         -1.7466,  7.6704, -5.3792, -4.8050,  5.6332,  5.2821, -7.3533,\n",
      "         -8.4412, -2.6318,  1.5244, -3.1069, -3.5218, -3.2928,  2.6249,\n",
      "         -8.9379,  5.1661, -1.4738, -0.4467,  3.9864,  6.0159, -1.4377,\n",
      "          4.6154,  1.5594, -3.7600, -7.4305,  8.9873,  1.0470, -9.3754,\n",
      "         -5.9316,  3.6541,  4.2636,  2.6972,  9.4112,  5.2886,  9.7714,\n",
      "         -3.6675,  8.1490,  8.0490, -1.2620, -8.9102, -8.3692,  2.3428,\n",
      "         -6.5806,  4.5461, -1.2401, -5.9931, -2.8880,  0.9935, -8.0787,\n",
      "          6.3836, -1.4067, -5.2798,  1.3477,  2.7418,  0.2399, -4.1840,\n",
      "          2.2556,  3.9286],\n",
      "        [ 4.9567,  6.7337,  1.0233, -8.9311, -8.8157, -0.0009, -8.9271,\n",
      "          0.1529, -7.9495, -0.1694, -2.9700, -9.8346,  1.9028, -6.1343,\n",
      "          1.1307, -3.7960,  9.6773, -5.2920, -8.4217,  7.3010,  2.5724,\n",
      "          8.4769,  5.6790, -2.0447, -8.6150,  9.1589, -4.0808,  3.1867,\n",
      "         -2.5976, -3.4682,  4.4506, -5.2895, -9.6059, -0.1700,  0.4952,\n",
      "          9.6355, -1.3327, -0.8760, -0.9454, -6.8562,  4.3180, -5.4786,\n",
      "          5.2676,  2.2141, -1.6474, -4.3306, -6.1045, -1.5274, -5.2854,\n",
      "          7.8693, -5.6501, -7.7225, -7.2508, -8.8649, -7.5343, -8.9224,\n",
      "         -8.8888,  2.8711, -8.3053, -8.6701,  5.6263,  9.8022,  4.2973,\n",
      "          8.9031, -0.6254, -8.7765, -3.0383,  2.7834,  2.0882,  5.7526,\n",
      "          0.9788, -5.7905, -9.1587,  0.0671, -8.9780, -0.0287,  5.1349,\n",
      "          4.9069,  6.4290, -2.7156,  1.5193, -5.0277,  1.6448,  1.1216,\n",
      "          2.8808, -4.1629,  3.4347, -4.0656,  2.6510, -4.9070,  4.1426,\n",
      "         -5.7657,  2.1411,  7.3516,  0.9436, -0.1633, -2.3007,  7.8194,\n",
      "          8.8658,  4.2812],\n",
      "        [ 3.3001, -8.9308, -8.5026, -8.8442, -3.7716, -2.9774,  4.3268,\n",
      "          4.4021, -9.0399,  7.4469,  5.4289,  6.9796, -8.8773, -6.6610,\n",
      "          3.2503, -8.0399,  2.8378, -8.9418, -7.3891,  1.4908, -5.3005,\n",
      "          9.4997,  8.7373,  4.7037,  6.3946,  3.7551,  7.0192, -6.8910,\n",
      "         -2.8359, -6.5339,  6.0997, -9.8854,  2.1365, -2.0884,  0.2213,\n",
      "         -3.0836,  8.7841, -7.3499,  4.5653,  5.1986,  0.9251,  6.0704,\n",
      "          3.6593,  6.4272, -5.8056,  4.3590,  6.6733,  2.5483,  9.1523,\n",
      "          4.2969,  2.2322,  0.7616,  8.3382, -6.1142, -9.9075, -3.0139,\n",
      "          2.7900,  7.6499,  8.8085,  1.6220,  7.1999, -1.4864, -3.6597,\n",
      "         -4.1384,  6.2247,  3.1229,  4.7278,  7.3702, -5.0886,  9.0308,\n",
      "         -3.9492,  7.9265,  8.2100,  4.0198, -7.1476,  7.9759, -5.8194,\n",
      "          6.3992, -2.0251, -6.8627,  8.0047, -4.8898, -8.1035,  6.8529,\n",
      "          9.9021, -8.0369, -5.0325, -3.6979,  1.1689,  5.2919, -9.6395,\n",
      "         -0.1269,  7.1448, -9.5934,  1.3526, -8.3424,  9.0479, -4.7843,\n",
      "          3.6890, -5.1910]])), ('output.bias', tensor([-0.1009, -0.2194,  0.1669,  0.0456, -0.0865,  0.0160,  0.0349,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        -0.0638, -0.0423, -0.1113]))])}\n",
      "end checkpoint print\n",
      "Network(\n",
      "  (hidden_layers): ModuleList(\n",
      "    (0): Linear(in_features=784, out_features=400, bias=True)\n",
      "    (1): Linear(in_features=400, out_features=200, bias=True)\n",
      "    (2): Linear(in_features=200, out_features=100, bias=True)\n",
      "  )\n",
      "  (output): Linear(in_features=100, out_features=10, bias=True)\n",
      "  (dropout): Dropout(p=0.5)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = load_checkpoint('checkpoint.pth')\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
